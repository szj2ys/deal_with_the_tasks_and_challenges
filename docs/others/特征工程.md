
## 连续数据离散化

```python
# 对Namelen 字段进行处理
df['Namelen'] = pd.cut(df['Namelen'], bins=[0, 20, 30, 40, 50, 100], labels=[0, 1, 2, 3, 4])
# 对Numbers 字段进行处理,分别为一个人、两个人、三个人和多个人
df['Numbers'] = pd.cut(df[''], bins=[0, 1, 2, 3, 20], labels=[0, 1, 2, 3])
```

## 类别编码

### Onehot


在计算机中需要计算不同特征之间的尺度，例如性别中的male和female，计算机是
无法直接计算两个特征，但是你如果将male表示1，female表示2，下次计算机遇到
了就会直接用2-1=1表示距离这样做的好处是计算机可以识别，并且可以快速的计算


```python
for feature in ['Sex', 'Embarked', 'CabinType', 'AgeType', ]:
    le = preprocessing.LabelEncoder()
    le.fit(df[feature])
    df[feature] = le.transform(df[feature])
```

### LabelEncoder


```python
pass
```

## 数据进行累加
```python
df['sum_age'] = df['age'].cumsum()
```

## 归一化
会改变数据的分布，归一化使得异常值对最终结果不会造成更大的影响

### 方法一
```python
max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))

df[['use_dates_month']].apply(max_min_scaler)
```
### 方法二
```python
from sklearn.preprocessing import MinMaxScaler

#实现归一化
scalar = MinMaxScaler()  #实例化
df['open_scaller'] = scalar.fit_transform(df[['open']])
```
> 注意在特定场景下最大值最小值是变化的， 另外，最大值与最小值非常容易受异常点影响 ，所以这种方法鲁棒性较差，只适合传统精确小数据场景。

## 标准化
通过对原始数据进行变换把数据变换到均值为0，方差为1范围内，不改变数据的分布
![](https://img-blog.csdnimg.cn/20190610103457804.png)

```python
from sklearn.preprocessing import StandardScaler

std = StandardScaler()
df['pct_chg_std'] = scalar.fit_transform(df[['pct_chg']])
```
>在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景

## 归一化、谈标准化比较：

- 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变

- 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。

## 数据相关性

```python
# 相关系数矩阵，即给出了任意两款菜式之间的相关系数
df.corr()
# 显示“百合酱蒸凤爪”与其他菜式的相关系数
df.corr()[u'百合酱蒸凤爪']
# 计算“百合酱蒸凤爪”与“翡翠蒸香茜饺”的相关系数
df[u'百合酱蒸凤爪'].corr(df[u'翡翠蒸香茜饺'])
```







