
## 连续数据离散化

```python
# 对Namelen 字段进行处理
df['Namelen'] = pd.cut(df['Namelen'], bins=[0, 20, 30, 40, 50, 100], labels=[0, 1, 2, 3, 4])
# 对Numbers 字段进行处理,分别为一个人、两个人、三个人和多个人
df['Numbers'] = pd.cut(df[''], bins=[0, 1, 2, 3, 20], labels=[0, 1, 2, 3])
```

## 类别编码

### Onehot


```python
pass
```


### LabelEncoder
在计算机中需要计算不同特征之间的尺度，例如性别中的male和female，计算机是
无法直接计算两个特征，但是你如果将male表示1，female表示2，下次计算机遇到
了就会直接用2-1=1表示距离这样做的好处是计算机可以识别，并且可以快速的计算


```python
for feature in ['Sex', 'Embarked', 'CabinType', 'AgeType', ]:
    le = preprocessing.LabelEncoder()
    le.fit(df[feature])
    df[feature] = le.transform(df[feature])
```

## 数据变换
数据变换，这个操作在特征工程中用得还是蛮多的，一个特征在当前的分布下无法有明显的区分度，但一个小小的变换则可以带来意想不到的效果
### 多项式变换
按照指定的degree，进行多项式操作从而衍生出新变量(当然这是针对每一列特征内的操作)
举个栗子：
```python
from sklearn.datasets import load_iris  
#导入IRIS数据集  
iris = load_iris()
iris.data[0]

# Output: array([ 5.1,  3.5,  1.4,  0.2])
```
```python
tt[0]

# Output: array([  1.  ,   5.1 ,   3.5 ,   1.4 ,   0.2 ,  26.01,  17.85,   7.14, 1.02,  12.25,   4.9 ,   0.7 ,   1.96,   0.28,   0.04])
```
因为PolynomialFeatures()方法默认degree是2，所以只会进行二项式的衍生。

一般来说，多项式变换都是按照下面的方式来的：

f = kx + b   一次函数（degree为1）

f = ax^2 + b*x + w  二次函数（degree为2）

f = a*x^3 + b*x^2 + c*x + w  三次函数（degree为3）



这类的转换可以适当地提升模型的拟合能力，对于在线性回归模型上的应用较为广泛。

```python
from sklearn.datasets import load_iris  
#导入IRIS数据集  
iris = load_iris()

#多项式转换  
#参数degree为度，默认值为2 
from sklearn.preprocessing import PolynomialFeatures  
PolynomialFeatures().fit_transform(iris.data) 
```

### 对数变换

这个操作就是直接进行一个对数转换，改变原先的数据分布，而可以达到的作用主要有:

1）取完对数之后可以缩小数据的绝对数值，方便计算；

2）取完对数之后可以把乘法计算转换为加法计算；

3）还有就是分布改变带来的意想不到的效果；



numpy库里就有好几类对数转换的方法，可以通过from numpy import xxx 进行导入使用。

- log：计算自然对数

- log10：底为10的log

- log2：底为2的log

- log1p：底为e的log

```python
from sklearn.datasets import load_iris  
#导入IRIS数据集  
iris = load_iris() 

#对数变换
from numpy import log1p  
from sklearn.preprocessing import FunctionTransformer  
#自定义转换函数为对数函数的数据变换  
#第一个参数是单变元函数  
FunctionTransformer(log1p).fit_transform(iris.data) 
```

## 数据进行累加
```python
df['sum_age'] = df['age'].cumsum()
```

## 无量纲化
> 无量纲化：即nondimensionalize 或者dimensionless，是指通过一个合适的变量替代，将一个涉及物理量的方程的部分或全部的单位移除，以求简化实验或者计算的目的。——百度百科



进行进一步解释，比如有两个字段，一个是车行走的公里数，另一个是人跑步的距离，他们之间的单位其实差异还是挺大的，其实两者之间无法进行比较的，但是我们可以进行去量纲，把他们的变量值进行缩放，都统一到某一个区间内，比如0-1，便于不同单位或者量级之间的指标可以进行比较or加权！

下面的是sklearn里的一些无量纲化的常见操作方法。

```python
from sklearn.datasets import load_iris  
#导入IRIS数据集  
iris = load_iris()

#标准化，返回值为标准化后的数据  
from sklearn.preprocessing import StandardScaler  
StandardScaler().fit_transform(iris.data)  

#区间缩放，返回值为缩放到[0, 1]区间的数据  
from sklearn.preprocessing import MinMaxScaler  
MinMaxScaler().fit_transform(iris.data)  

#归一化，返回值为归一化后的数据
from sklearn.preprocessing import Normalizer  
Normalizer().fit_transform(iris.data)
```


## 归一化
会改变数据的分布，归一化使得异常值对最终结果不会造成更大的影响

### 方法一
```python
max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))

df[['use_dates_month']].apply(max_min_scaler)
```
### 方法二
```python
from sklearn.preprocessing import MinMaxScaler

#实现归一化
scalar = MinMaxScaler()  #实例化
df['open_scaller'] = scalar.fit_transform(df[['open']])
```
> 注意在特定场景下最大值最小值是变化的， 另外，最大值与最小值非常容易受异常点影响 ，所以这种方法鲁棒性较差，只适合传统精确小数据场景。

## 标准化
通过对原始数据进行变换把数据变换到均值为0，方差为1范围内，不改变数据的分布
![](https://img-blog.csdnimg.cn/20190610103457804.png)

```python
from sklearn.preprocessing import StandardScaler

std = StandardScaler()
df['pct_chg_std'] = scalar.fit_transform(df[['pct_chg']])
```
>在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景

## 归一化、谈标准化比较：

- 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变

- 对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小。

## 数据相关性

```python
# 相关系数矩阵，即给出了任意两款菜式之间的相关系数
df.corr()
# 显示“百合酱蒸凤爪”与其他菜式的相关系数
df.corr()[u'百合酱蒸凤爪']
# 计算“百合酱蒸凤爪”与“翡翠蒸香茜饺”的相关系数
df[u'百合酱蒸凤爪'].corr(df[u'翡翠蒸香茜饺'])
```







